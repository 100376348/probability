{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Latent_Space_Models_TFP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "ldRhFo81WscB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "metadata": {
        "id": "VMOWFkQfWsMF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NtUBlTmlUCE9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Latent Space Models for Neural Data with TFP\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://drive.google.com/file/d/18Vs8j8SYrO9jw6bPK8c4JlAS5uE6Kwat/view?usp=sharing\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "Original content [this Repository](https://github.com/blei-lab/edward) and [this tutorial](http://edwardlib.org/tutorials/latent-space-models), created by [the Blei Lab](http://www.cs.columbia.edu/~blei/). The initial version of this tutorial was written by Maja Rudolph.\n",
        "\n",
        "Ported to Tensorflow Probability by Matthew McAteer ([`@MatthewMcAteer0`](https://twitter.com/MatthewMcAteer0)), with help from the TFP team at  Google ([`tfprobability@tensorflow.org`](mailto:tfprobability@tensorflow.org)).\n",
        "\n",
        "---\n",
        "\n",
        ">[Dependencies & Prerequisites](#scrollTo=2ZtWUjXYRXQi)\n",
        "\n",
        ">[Introduction](#scrollTo=2ZtWUjXYRXQi)\n",
        "\n",
        ">>[Data](#scrollTo=2ZtWUjXYRXQi)\n",
        "\n",
        ">>[Model](#scrollTo=2ZtWUjXYRXQi)\n",
        "\n",
        ">>[Inference](#scrollTo=2ZtWUjXYRXQi)\n",
        "\n",
        ">>[Criticism](#scrollTo=2ZtWUjXYRXQi)\n",
        "\n",
        ">[References](#scrollTo=2ZtWUjXYRXQi)"
      ]
    },
    {
      "metadata": {
        "id": "TaIoLupxbekc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dependencies & Prerequisites"
      ]
    },
    {
      "metadata": {
        "id": "komvNIt0UGEQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install -q tfp-nightly\n",
        "!pip3 install -q observations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0zc2vb6EUCFA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# import edward as ed\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# from edward.models import Normal, Poisson\n",
        "from observations import celegans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DAlrm2vXkZ0p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def session_options(enable_gpu_ram_resizing=True, enable_xla=True):\n",
        "    \"\"\"\n",
        "    Allowing the notebook to make use of GPUs if they're available.\n",
        "    \n",
        "    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear \n",
        "    algebra that optimizes TensorFlow computations.\n",
        "    \"\"\"\n",
        "    config = tf.ConfigProto()\n",
        "    config.log_device_placement = True\n",
        "    if enable_gpu_ram_resizing:\n",
        "        # `allow_growth=True` makes it possible to connect multiple colabs to your\n",
        "        # GPU. Otherwise the colab malloc's all GPU ram.\n",
        "        config.gpu_options.allow_growth = True\n",
        "    if enable_xla:\n",
        "        # Enable on XLA. https://www.tensorflow.org/performance/xla/.\n",
        "        config.graph_options.optimizer_options.global_jit_level = (\n",
        "            tf.OptimizerOptions.ON_1)\n",
        "    return config\n",
        "\n",
        "\n",
        "def reset_sess(config=None):\n",
        "    \"\"\"\n",
        "    Convenience function to create the TF graph & session or reset them.\n",
        "    \"\"\"\n",
        "    if config is None:\n",
        "        config = session_options()\n",
        "    global sess\n",
        "    tf.reset_default_graph()\n",
        "    try:\n",
        "        sess.close()\n",
        "    except:\n",
        "        pass\n",
        "    sess = tf.InteractiveSession(config=config)\n",
        "\n",
        "    \n",
        "def evaluate(tensors):\n",
        "    \"\"\"\n",
        "    A \"Universal\" evaluate function for both running either Graph mode (default)\n",
        "    or Eager mode (https://www.tensorflow.org/guide/eager) in Tensorflow.\n",
        "    \"\"\"\n",
        "    if context.executing_eagerly():\n",
        "        return (t.numpy() for t in tensprs)\n",
        "    with tf.get_default_session() as sess:\n",
        "        return sess.run(tensors)\n",
        "\n",
        "reset_sess()\n",
        "\n",
        "\n",
        "def strip_consts(graph_def, max_const_size=32):\n",
        "  \"\"\"\n",
        "  Strip large constant values from graph_def.\n",
        "  \"\"\"\n",
        "  strip_def = tf.GraphDef()\n",
        "  for n0 in graph_def.node:\n",
        "    n = strip_def.node.add()\n",
        "    n.MergeFrom(n0)\n",
        "    if n.op == 'Const':\n",
        "      tensor = n.attr['value'].tensor\n",
        "      size = len(tensor.tensor_content)\n",
        "      if size > max_const_size:\n",
        "        tensor.tensor_content = bytes(\"<stripped %d bytes>\"%size, 'utf-8')\n",
        "  return strip_def\n",
        "\n",
        "\n",
        "def draw_graph(model, *args, **kwargs):\n",
        "  \"\"\"\n",
        "  Visualize TensorFlow graph.\n",
        "  \"\"\"\n",
        "  graph = tf.Graph()\n",
        "  with graph.as_default():\n",
        "    model(*args, **kwargs)\n",
        "  graph_def = graph.as_graph_def()\n",
        "  strip_def = strip_consts(graph_def, max_const_size=32)\n",
        "  code = \"\"\"\n",
        "      <script>\n",
        "        function load() {{\n",
        "          document.getElementById(\"{id}\").pbtxt = {data};\n",
        "        }}\n",
        "      </script>\n",
        "      <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
        "      <div style=\"height:600px\">\n",
        "        <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
        "      </div>\n",
        "  \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
        "\n",
        "  iframe = \"\"\"\n",
        "      <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
        "  \"\"\".format(code.replace('\"', '&quot;'))\n",
        "  IPython.display.display(IPython.display.HTML(iframe))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6sGkB9-DYote",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Many scientific fields involve the study of network data, including social networks, networks in statistical physics, biological networks, and information networks (Goldenberg, Zheng, Fienberg, & Airoldi, 2010; Newman, 2010).\n",
        "\n",
        "What we can learn about nodes in a network from their connectivity patterns? We can begin to study this using a latent space model (Hoff, Raftery, & Handcock, 2002). Latent space models embed nodes in the network in a latent space, where the likelihood of forming an edge between two nodes depends on their distance in the latent space.\n",
        "\n",
        "We will analyze network data from neuroscience."
      ]
    },
    {
      "metadata": {
        "id": "zy8CBHKTUCFH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "The data comes from [Mark Newman's repository](http://www-personal.umich.edu/~mejn/netdata/).\n",
        "It is a weighted, directed network representing the neural network of\n",
        "the nematode\n",
        "[C. Elegans](https://en.wikipedia.org/wiki/Caenorhabditis_elegans)\n",
        "compiled by Watts & Strogatz (1998) using experimental data\n",
        "by White, Southgate, Thomson, & Brenner (1986).\n",
        "\n",
        "The neural network consists of around $300$ neurons. Each connection\n",
        "between neurons\n",
        "is associated with a weight (positive integer) capturing the strength\n",
        "of the connection.\n",
        "\n",
        "First, we load the data."
      ]
    },
    {
      "metadata": {
        "id": "wQITpMxvUCFI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = celegans(\"~/data\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OPDx03R4UCFL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "What can we learn about the neurons from their connectivity patterns? Using\n",
        "a latent space model (Hoff et al., 2002), we will learn a latent\n",
        "embedding for each neuron to capture the similarities between them.\n",
        "\n",
        "Each neuron $n$ is a node in the network and is associated with a latent\n",
        "position $z_n\\in\\mathbb{R}^K$.\n",
        "We place a Gaussian prior on each of the latent positions.\n",
        "\n",
        "The log-odds of an edge between node $i$ and\n",
        "$j$ is proportional to the Euclidean distance between the latent\n",
        "representations of the nodes $|z_i- z_j|$. Here, we\n",
        "model the weights ($Y_{ij}$) of the edges with a Poisson likelihood.\n",
        "The rate is the reciprocal of the distance in latent space. The\n",
        "generative process is as follows:\n",
        "\n",
        "1. \n",
        "For each node $n=1,\\ldots,N$,\n",
        "\\begin{align}\n",
        "z_n \\sim N(0,I).\n",
        "\\end{align}\n",
        "2. \n",
        "For each edge $(i,j)\\in\\{1,\\ldots,N\\}\\times\\{1,\\ldots,N\\}$,\n",
        "\\begin{align}\n",
        "Y_{ij} \\sim \\text{Poisson}\\Bigg(\\frac{1}{|z_i - z_j|}\\Bigg).\n",
        "\\end{align}\n",
        "\n",
        "In TFP, we write the model as follows."
      ]
    },
    {
      "metadata": {
        "id": "LDg5KGCIUCFM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N = x_train.shape[0]  # number of data points\n",
        "K = 3  # latent dimensionality\n",
        "\n",
        "z = Normal(loc=tf.zeros([N, K]), scale=tf.ones([N, K]))\n",
        "\n",
        "# Calculate N x N distance matrix.\n",
        "# 1. Create a vector, [||z_1||^2, ||z_2||^2, ..., ||z_N||^2], and tile\n",
        "# it to create N identical rows.\n",
        "xp = tf.tile(tf.reduce_sum(tf.pow(z, 2), 1, keep_dims=True), [1, N])\n",
        "# 2. Create a N x N matrix where entry (i, j) is ||z_i||^2 + ||z_j||^2\n",
        "# - 2 z_i^T z_j.\n",
        "xp = xp + tf.transpose(xp) - 2 * tf.matmul(z, z, transpose_b=True)\n",
        "# 3. Invert the pairwise distances and make rate along diagonals to\n",
        "# be close to zero.\n",
        "xp = 1.0 / tf.sqrt(xp + tf.diag(tf.zeros(N) + 1e3))\n",
        "\n",
        "x = Poisson(rate=xp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TT2AdLnVUCFP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "Maximum a posteriori (MAP) estimation is simple in Edward. Two lines are\n",
        "required: Instantiating inference and running it."
      ]
    },
    {
      "metadata": {
        "id": "BwCUhtETUCFQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# No MAP\n",
        "inference = ed.MAP([z], data={x: x_train})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VlAV1iOXUCFT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "See this extended tutorial about\n",
        "[MAP estimation in Edward](http://edwardlib.org/tutorials/map).\n",
        "\n",
        "One could instead run variational inference. This requires specifying\n",
        "a variational model and instantiating `KLqp`."
      ]
    },
    {
      "metadata": {
        "id": "YlPOm5WRpgPb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "# Assumes user supplies `likelihood`, `prior`, `surrogate_posterior`\n",
        "# functions and that each returns a \n",
        "# tf.distribution.Distribution-like object.\n",
        "elbo_loss = tfp.vi.monte_carlo_csiszar_f_divergence(\n",
        "    f=tfp.vi.kl_reverse,  # Equivalent to \"Evidence Lower BOund\"\n",
        "    p_log_prob=lambda z: likelihood(z).log_prob(x) + prior().log_prob(z),\n",
        "    q=surrogate_posterior(x),\n",
        "    num_draws=1)\n",
        "train = tf.train.AdamOptimizer(\n",
        "    learning_rate=0.01).minimize(elbo_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7QNNzojGUCFU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Alternatively, run Variational inference with the KL divergence\n",
        "qz = Normal(loc=tf.get_variable(\"qz/loc\", [N * K]),\n",
        "            scale=tf.nn.softplus(tf.get_variable(\"qz/scale\", [N * K])))\n",
        "inference = ed.KLqp({z: qz}, data={x: x_train})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B1UANH0KUCFX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "See this extended tutorial about\n",
        "[variational inference in Edward](http://edwardlib.org/tutorials/variational-inference).\n",
        "\n",
        "Finally, the following line runs the inference procedure for 2500\n",
        "iterations."
      ]
    },
    {
      "metadata": {
        "id": "n9GSua9XUCFX",
        "colab_type": "code",
        "colab": {},
        "outputId": "12d50202-1bf9-47dc-d507-3994f77e9972"
      },
      "cell_type": "code",
      "source": [
        "# Add variational inference from TFP example\n",
        "inference.run(n_iter=2500)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2500/2500 [100%] ██████████████████████████████ Elapsed: 11s | Loss: 35984.855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EDSYnetrlkKg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Visualizing the graph we've constructed\n",
        "# draw_graph(linear_mixed_effects_model, features_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2iRWNyBxUCFb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "1. Goldenberg, A., Zheng, A. X., Fienberg, S. E., & Airoldi, E. M. (2010). [A survey of statistical network models.](https://www.nowpublishers.com/article/Details/MAL-005) Foundations and Trends in Machine Learning.\n",
        "2. Hoff, P. D., Raftery, A. E., & Handcock, M. S. (2002). [Latent space approaches to social network analysis.](https://amstat.tandfonline.com/doi/abs/10.1198/016214502388618906#.W2X9PN-YW8g) Journal of the American Statistical Association, 97(460), 1090–1098.\n",
        "3. Newman, M. (2010). [Networks: An introduction.](https://books.google.com/books?hl=en&lr=&id=YdZjDwAAQBAJ&oi=fnd&pg=PP1&dq=networks+an+introduction+newman&ots=V_IZ1Qkcrv&sig=e32XDZs8gzBAHzsBGEYVmWQhsis#v=onepage&q=networks%20an%20introduction%20newman&f=false) Oxford University Press.\n",
        "4. Watts, D. J., & Strogatz, S. H. (1998). [Collective dynamics of ‘small-world’networks.](https://www.nature.com/articles/30918) Nature, 393(6684), 440–442.\n",
        "5. White, J. G., Southgate, E., Thomson, J. N., & Brenner, S. (1986). [The structure of the nervous system of the nematode caenorhabditis elegans.](https://pdfs.semanticscholar.org/0bb2/1a76b5604211927cbc3c18f64437adbd834a.pdf) Philos Trans R Soc Lond B Biol Sci, 314(1165), 1–340."
      ]
    },
    {
      "metadata": {
        "id": "GbwzzzGkUCFb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.core.display import HTML\n",
        "def css_styling():\n",
        "    styles = open(\"../styles/custom.css\", \"r\").read()\n",
        "    return HTML(styles)\n",
        "css_styling()\n",
        "\n",
        "#  \"#F15854\",  // red\n",
        "#  \"#5DA5DA\",  // blue\n",
        "#  \"#FAA43A\",  // orange\n",
        "#  \"#60BD68\",  // green\n",
        "#  \"#F17CB0\",  // pink\n",
        "#  \"#B2912F\",  // brown\n",
        "#  \"#B276B2\",  // purple\n",
        "#  \"#DECF3F\",  // yellow\n",
        "#  \"#4D4D4D\",  // gray\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}