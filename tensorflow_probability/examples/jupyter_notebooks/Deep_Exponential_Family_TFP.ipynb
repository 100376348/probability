{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Exponential_Family_TFP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "X8rXpneSaAsU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "metadata": {
        "id": "oXEdGST4aAfX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ECsuwM84RHHm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Exponential Family with TFP\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://drive.google.com/file/d/1LiIZct-3Qk0yd7EkypjZABwcSPyeyhd0/view?usp=sharing\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "Original content [this Repository](https://github.com/blei-lab/edward) and [this paper](http://www.cs.toronto.edu/~lcharlin/papers/def_aistats.pdf), created by [the Blei Lab](http://www.cs.columbia.edu/~blei/)\n",
        "\n",
        "Ported to Tensorflow Probability by Matthew McAteer ([`@MatthewMcAteer0`](https://twitter.com/MatthewMcAteer0)), with help from the TFP team at  Google ([`tfprobability@tensorflow.org`](mailto:tfprobability@tensorflow.org)).\n",
        "\n",
        "---\n",
        "\n",
        ">[Dependencies & Prerequisites](#scrollTo=2ZtWUjXYRXQi)\n",
        "\n",
        ">[Introduction](#scrollTo=2ZtWUjXYRXQi)\n",
        "\n",
        ">>[Data](#scrollTo=2ZtWUjXYRXQi)\n",
        "\n",
        ">>[Model](#scrollTo=2ZtWUjXYRXQi)\n",
        "\n",
        ">>[Inference](#scrollTo=2ZtWUjXYRXQi)\n",
        "\n",
        ">>[Criticism](#scrollTo=2ZtWUjXYRXQi)\n",
        "\n",
        ">[References](#scrollTo=2ZtWUjXYRXQi)"
      ]
    },
    {
      "metadata": {
        "id": "YJF4CVOaXeMT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dependencies & Prerequisites"
      ]
    },
    {
      "metadata": {
        "id": "S65MUPJgZt8P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install -q tfp-nightly\n",
        "!pip3 install -q observations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jv-uVD3UXdta",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# import edward as ed\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from datetime import datetime\n",
        "# from edward.models import Gamma, Poisson, Normal, PointMass, TransformedDistribution\n",
        "# from edward.util import Progbar\n",
        "from observations import nips"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fwEJTjTDkim4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def session_options(enable_gpu_ram_resizing=True, enable_xla=True):\n",
        "    \"\"\"\n",
        "    Allowing the notebook to make use of GPUs if they're available.\n",
        "    \n",
        "    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear \n",
        "    algebra that optimizes TensorFlow computations.\n",
        "    \"\"\"\n",
        "    config = tf.ConfigProto()\n",
        "    config.log_device_placement = True\n",
        "    if enable_gpu_ram_resizing:\n",
        "        # `allow_growth=True` makes it possible to connect multiple colabs to your\n",
        "        # GPU. Otherwise the colab malloc's all GPU ram.\n",
        "        config.gpu_options.allow_growth = True\n",
        "    if enable_xla:\n",
        "        # Enable on XLA. https://www.tensorflow.org/performance/xla/.\n",
        "        config.graph_options.optimizer_options.global_jit_level = (\n",
        "            tf.OptimizerOptions.ON_1)\n",
        "    return config\n",
        "\n",
        "\n",
        "def reset_sess(config=None):\n",
        "    \"\"\"\n",
        "    Convenience function to create the TF graph & session or reset them.\n",
        "    \"\"\"\n",
        "    if config is None:\n",
        "        config = session_options()\n",
        "    global sess\n",
        "    tf.reset_default_graph()\n",
        "    try:\n",
        "        sess.close()\n",
        "    except:\n",
        "        pass\n",
        "    sess = tf.InteractiveSession(config=config)\n",
        "\n",
        "    \n",
        "def evaluate(tensors):\n",
        "    \"\"\"\n",
        "    A \"Universal\" evaluate function for both running either Graph mode (default)\n",
        "    or Eager mode (https://www.tensorflow.org/guide/eager) in Tensorflow.\n",
        "    \"\"\"\n",
        "    if context.executing_eagerly():\n",
        "        return (t.numpy() for t in tensprs)\n",
        "    with tf.get_default_session() as sess:\n",
        "        return sess.run(tensors)\n",
        "\n",
        "reset_sess()\n",
        "\n",
        "\n",
        "def strip_consts(graph_def, max_const_size=32):\n",
        "  \"\"\"\n",
        "  Strip large constant values from graph_def.\n",
        "  \"\"\"\n",
        "  strip_def = tf.GraphDef()\n",
        "  for n0 in graph_def.node:\n",
        "    n = strip_def.node.add()\n",
        "    n.MergeFrom(n0)\n",
        "    if n.op == 'Const':\n",
        "      tensor = n.attr['value'].tensor\n",
        "      size = len(tensor.tensor_content)\n",
        "      if size > max_const_size:\n",
        "        tensor.tensor_content = bytes(\"<stripped %d bytes>\"%size, 'utf-8')\n",
        "  return strip_def\n",
        "\n",
        "\n",
        "def draw_graph(model, *args, **kwargs):\n",
        "  \"\"\"\n",
        "  Visualize TensorFlow graph.\n",
        "  \"\"\"\n",
        "  graph = tf.Graph()\n",
        "  with graph.as_default():\n",
        "    model(*args, **kwargs)\n",
        "  graph_def = graph.as_graph_def()\n",
        "  strip_def = strip_consts(graph_def, max_const_size=32)\n",
        "  code = \"\"\"\n",
        "      <script>\n",
        "        function load() {{\n",
        "          document.getElementById(\"{id}\").pbtxt = {data};\n",
        "        }}\n",
        "      </script>\n",
        "      <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
        "      <div style=\"height:600px\">\n",
        "        <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
        "      </div>\n",
        "  \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
        "\n",
        "  iframe = \"\"\"\n",
        "      <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
        "  \"\"\".format(code.replace('\"', '&quot;'))\n",
        "  IPython.display.display(IPython.display.HTML(iframe))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fYTUqdHKXipl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Sparse Gamma deep exponential family (Ranganath et al., 2015).\n",
        "\n",
        "### Abstract\n",
        "We describe deep exponential families (DEFs), a class of latent variable models that are inspired by the hidden structures used in deep neural networks. DEFs capture a hierarchy of dependencies between latent variables, and are easily generalized to many settings through exponential families. We perform inference using recent “black box” variational inference techniques. We then evaluate various DEFs on text and combine multiple DEFs into a model for pairwise recommendation data. In an extensive study, we show going beyond one layer improves predictions for DEFs. We demonstrate that DEFs find interesting exploratory structure in large data sets, and give better predictive\n",
        "performance than state-of-the-art models.\n",
        "\n",
        "---\n",
        "\n",
        "In this paper we develop deep exponential families\n",
        "(DEFs), a flexible family of probability distributions\n",
        "that reflect the intuitions behind deep unsupervised\n",
        "feature learning. In a DEF, observations arise from\n",
        "a cascade of layers of latent variables. Each layer’s\n",
        "variables are drawn from an exponential family that is\n",
        "governed by the inner product of the previous layer’s\n",
        "variables and a set of weights.\n",
        "As in deep unsupervised feature learning, a DEF represents\n",
        "hidden patterns, from coarse to fine grained,\n",
        "that compose with each other to form the observations.\n",
        "DEFs also enjoy the advantages of probabilistic\n",
        "modeling. Through their connection to exponential\n",
        "families [7], they support many kinds of data. Overall\n",
        "DEFs combine the powerful representations of deep\n",
        "networks with the flexibility of graphical models.\n",
        "Consider the problem of modeling documents. We can\n",
        "represent a document as a vector of term counts modeled\n",
        "with Poisson random variables [9]. In one type of\n",
        "DEF, the rate of each term’s Poisson count is an inner\n",
        "product of a layer of latent variables (one level up from\n",
        "the terms) and a set of weights that are shared across\n",
        "documents. Loosely, we can think of the latent layer\n",
        "the observations as per-document “topic” activations,\n",
        "each of which ignites a set of related terms via their inner\n",
        "product with the weights. These latent topics are,\n",
        "in turn, modeled in a similar way, conditioned on a\n",
        "layer above of “super topics.” Just as the topics group\n",
        "related terms, the super topics group related topics,\n",
        "again via the inner product.\n",
        "Figure 1 illustrates an example of a three level DEF\n",
        "uncovered from a large set of articles in The New York\n",
        "Times. (This style of model, though with different details,\n",
        "has been previously studied in the topic modeling\n",
        "literature [21].) Conditional on the word counts\n",
        "of the articles, the DEF defines a posterior distribution\n",
        "of the per-document cascades of latent variables\n",
        "and the layers of weights. Here we have visualized\n",
        "two third-layer topics which correspond to the concepts\n",
        "of “Government” and “Politics”. We focus on\n",
        "“Government” and notice that the model has discovered,\n",
        "through its second-layer super-topics, the three\n",
        "branches of government: judiciary ( left), legislative\n",
        "(center) and executive (right).\n",
        "This is just one example. In a DEF, the latent variables\n",
        "can be from any exponential family: Bernoulli\n",
        "latent variables recover the classical sigmoid belief\n",
        "network [25]; Gamma latent variables give something\n",
        "akin to deep version of nonnegative matrix factorization\n",
        "[20]; Gaussian latent variables lead to the types of\n",
        "models that have recently been explored in the context\n"
      ]
    },
    {
      "metadata": {
        "id": "pr6yQg-eRHHo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Our example\n",
        "\n",
        "We apply it as a topic model on the collection of NIPS 2011 conference\n",
        "papers. The loss function can sometimes erroneously output a negative value or NaN. This happens when the samples from the variational approximation\n",
        "are numerically zero, which causes Gamma log probs to output inf. With default settings (in particular, with log normal variational\n",
        "approximation), it takes ~62s per epoch on a Titan X (Pascal). The following results are on epoch 12.\n",
        "\n",
        "```\n",
        "10000/10000 [100%] ██████████████████████████████ Elapsed: 62s\n",
        "Negative log-likelihood <= -1060649.607\n",
        "Perplexity <= 0.205\n",
        "Topic 0: let distribution set strategy distributions given learning information use property\n",
        "Topic 1: functions problem risk function submodular cut level clustering sets performance\n",
        "Topic 2: action value learning regret reward actions algorithm optimal state return\n",
        "Topic 3: posterior stochastic approach information based using prior mean divergence since\n",
        "Topic 4: player inference game propagation experts static query expert base variables\n",
        "Topic 5: algorithm set loss weak algorithms optimal submodular online cost setting\n",
        "Topic 6: sparse sparsity norm solution learning penalty greedy structure wise regularization\n",
        "Topic 7: learning training linear kernel using coding accuracy performance dataset based\n",
        "Topic 8: object categories image features examples classes images class objects visual\n",
        "Topic 9: data manifold matrix points dimensional point low linear gradient optimization\n",
        "```\n",
        "\n",
        "A Gamma variational approximation produces worse results, which is\n",
        "likely due to the high variance in stochastic gradients. It takes ~2\n",
        "minutes per epoch on a Titan X (Pascal). Following results are on\n",
        "epoch 12.\n",
        "```\n",
        "Negative log-likelihood <= 3738025.615\n",
        "Perplexity <= 266.623\n",
        "Topic 0: reasons posterior tion using similar tools university input computed refers\n",
        "Topic 1: expected since much related rate defined optimization vector thus neurons\n",
        "Topic 2: large linear given table shown true drop classification constraints current\n",
        "Topic 3: proposed processing estimated better values gaussian form test true setting\n",
        "Topic 4: see methods local several rate processing general vector enables section\n",
        "Topic 5: thus case methods image dataset models different instead new respectively\n",
        "Topic 6: based consider samples step object see kernel since problem training\n",
        "Topic 7: approaches linear computing show gaussian data expected analysis well proof\n",
        "Topic 8: fig point kernel bayesian solution applications results follows regression computer\n",
        "Topic 9: conference optimization training pages maximum learning dataset performance state inference\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "-PXNFPEBRHHo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tf.flags.DEFINE_string(\"data_dir\", default=\"~/data\", help=\"\")\n",
        "# tf.flags.DEFINE_string(\"logdir\", default=\"~/log/def/\", help=\"\")\n",
        "# tf.flags.DEFINE_list(\"K\", default=[100, 30, 15], help=\"Number of components per layer.\")\n",
        "# tf.flags.DEFINE_string(\"q\", default=\"lognormal\", help=\"Choice of q; 'lognormal' or 'gamma'.\")\n",
        "# tf.flags.DEFINE_float(\"shape\", default=0.1, help=\"Gamma shape parameter.\")\n",
        "# tf.flags.DEFINE_float(\"lr\", default=1e-4, help=\"Learning rate step-size.\")\n",
        "# \n",
        "# FLAGS = tf.flags.FLAGS\n",
        "# FLAGS.data_dir = os.path.expanduser(FLAGS.data_dir)\n",
        "# FLAGS.logdir = os.path.expanduser(FLAGS.logdir)\n",
        "# timestamp = datetime.strftime(datetime.utcnow(), \"%Y%m%d_%H%M%S\")\n",
        "# FLAGS.logdir += timestamp + '_' + '_'.join([str(ks) for ks in FLAGS.K]) + \\\n",
        "#     '_q_' + str(FLAGS.q) + '_lr_' + str(FLAGS.lr)\n",
        "\n",
        "\n",
        "data_dir = \"~/data\"\n",
        "logdir = \"~/log/def/\"\n",
        "K =[100, 30, 15]  # Number of components per layer.\")\n",
        "q =\" lognormal\"   # Choice of q; 'lognormal' or 'gamma'.\")\n",
        "shape = 0.1       # Gamma shape parameter.\")\n",
        "lr = 1e-4         # Learning rate step-size.\")\n",
        "\n",
        "data_dir = os.path.expanduser(data_dir)\n",
        "logdir = os.path.expanduser(.logdir)\n",
        "timestamp = datetime.strftime(datetime.utcnow(), \"%Y%m%d_%H%M%S\")\n",
        "logdir += timestamp + '_' + '_'.join([str(ks) for ks in K]) + '_q_' + str(q) + '_lr_' + str(lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1GCZxouSRHHr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pointmass_q(shape, name=None):\n",
        "    with tf.variable_scope(name, default_name=\"pointmass_q\"):\n",
        "        min_mean = 1e-3\n",
        "        mean = tf.get_variable(\"mean\", shape)\n",
        "        rv = PointMass(tf.maximum(tf.nn.softplus(mean), min_mean))\n",
        "        return rv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c-XMTd0GRHHs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gamma_q(shape, name=None):\n",
        "    # Parameterize Gamma q's via shape and scale, with softplus unconstraints.\n",
        "    with tf.variable_scope(name, default_name=\"gamma_q\"):\n",
        "        min_shape = 1e-3\n",
        "        min_scale = 1e-5\n",
        "        shape = tf.get_variable(\n",
        "            \"shape\", shape,\n",
        "            initializer=tf.random_normal_initializer(mean=0.5, stddev=0.1))\n",
        "        scale = tf.get_variable(\n",
        "            \"scale\", shape, initializer=tf.random_normal_initializer(stddev=0.1))\n",
        "        rv = Gamma(tf.maximum(tf.nn.softplus(shape), min_shape),\n",
        "                   tf.maximum(1.0 / tf.nn.softplus(scale), 1.0 / min_scale))\n",
        "        return rv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l6Uuily3RHHv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lognormal_q(shape, name=None):\n",
        "    with tf.variable_scope(name, default_name=\"lognormal_q\"):\n",
        "        min_scale = 1e-5\n",
        "        loc = tf.get_variable(\"loc\", shape)\n",
        "        scale = tf.get_variable(\n",
        "            \"scale\", shape, initializer=tf.random_normal_initializer(stddev=0.1))\n",
        "        rv = TransformedDistribution(\n",
        "            distribution=Normal(loc, tf.maximum(tf.nn.softplus(scale), min_scale)),\n",
        "            bijector=tf.contrib.distributions.bijectors.Exp())\n",
        "        return rv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CZS2mICgRHHw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data"
      ]
    },
    {
      "metadata": {
        "id": "5D1DvivERHHx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ed.set_seed(42)\n",
        "\n",
        "x_train, metadata = nips(data_dir)\n",
        "documents = metadata['columns']\n",
        "words = metadata['rows']\n",
        "\n",
        "# Subset to documents in 2011 and words appearing in at least two\n",
        "# documents and have a total word count of at least 10.\n",
        "doc_idx = [i for i, document in enumerate(documents)\n",
        "             if document.startswith('2011')]\n",
        "documents = [documents[doc] for doc in doc_idx]\n",
        "x_train = x_train[:, doc_idx]\n",
        "word_idx = np.logical_and(np.sum(x_train != 0, 1) >= 2,\n",
        "                            np.sum(x_train, 1) >= 10)\n",
        "words = [word for word, idx in zip(words, word_idx) if idx]\n",
        "x_train = x_train[word_idx, :]\n",
        "x_train = x_train.T\n",
        "\n",
        "N = x_train.shape[0]  # number of documents\n",
        "D = x_train.shape[1]  # vocabulary size\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HfDHMZVcRHHz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model"
      ]
    },
    {
      "metadata": {
        "id": "zrXhfjr8RHHz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "W2 = Gamma(0.1, 0.3, sample_shape=[K[2], K[1]])\n",
        "W1 = Gamma(0.1, 0.3, sample_shape=[K[1], K[0]])\n",
        "W0 = Gamma(0.1, 0.3, sample_shape=[K[0], D])\n",
        "\n",
        "z3 = Gamma(0.1, 0.1, sample_shape=[N, K[2]])\n",
        "z2 = Gamma(shape, shape / tf.matmul(z3, W2))\n",
        "z1 = Gamma(shape, shape / tf.matmul(z2, W1))\n",
        "x = Poisson(tf.matmul(z1, W0))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6DI0JGM9RHH0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ]
    },
    {
      "metadata": {
        "id": "uj8JX-2NRHH2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "qW2 = pointmass_q(W2.shape)\n",
        "qW1 = pointmass_q(W1.shape)\n",
        "qW0 = pointmass_q(W0.shape)\n",
        "\n",
        "if FLAGS.q == 'gamma':\n",
        "    qz3 = gamma_q(z3.shape)\n",
        "    qz2 = gamma_q(z2.shape)\n",
        "    qz1 = gamma_q(z1.shape)\n",
        "else:\n",
        "    qz3 = lognormal_q(z3.shape)\n",
        "    qz2 = lognormal_q(z2.shape)\n",
        "    qz1 = lognormal_q(z1.shape)\n",
        "\n",
        "# We apply variational EM with E-step over local variables\n",
        "# and M-step to point estimate the global weight matrices.\n",
        "inference_e = ed.KLqp({z1: qz1, z2: qz2, z3: qz3},\n",
        "                        data={x: x_train, W0: qW0, W1: qW1, W2: qW2})\n",
        "inference_m = ed.MAP({W0: qW0, W1: qW1, W2: qW2},\n",
        "                       data={x: x_train, z1: qz1, z2: qz2, z3: qz3})\n",
        "\n",
        "optimizer_e = tf.train.RMSPropOptimizer(FLAGS.lr)\n",
        "optimizer_m = tf.train.RMSPropOptimizer(FLAGS.lr)\n",
        "kwargs = {'optimizer': optimizer_e,\n",
        "            'n_print': 100,\n",
        "            'logdir': FLAGS.logdir,\n",
        "            'log_timestamp': False}\n",
        "if FLAGS.q == 'gamma':\n",
        "    kwargs['n_samples'] = 30\n",
        "inference_e.initialize(**kwargs)\n",
        "inference_m.initialize(optimizer=optimizer_m)\n",
        "\n",
        "sess = ed.get_session()\n",
        "tf.global_variables_initializer().run()\n",
        "\n",
        "n_epoch = 20\n",
        "n_iter_per_epoch = 10000\n",
        "for epoch in range(n_epoch):\n",
        "    print(\"Epoch {}\".format(epoch))\n",
        "    nll = 0.0\n",
        "\n",
        "    pbar = Progbar(n_iter_per_epoch)\n",
        "    for t in range(1, n_iter_per_epoch + 1):\n",
        "        pbar.update(t)\n",
        "        info_dict_e = inference_e.update()\n",
        "        info_dict_m = inference_m.update()\n",
        "        nll += info_dict_e['loss']\n",
        "\n",
        "    # Compute perplexity averaged over a number of training iterations.\n",
        "    # The model's negative log-likelihood of data is upper bounded by\n",
        "    # the variational objective.\n",
        "    nll /= n_iter_per_epoch\n",
        "    perplexity = np.exp(nll / np.sum(x_train))\n",
        "    print(\"Negative log-likelihood <= {:0.3f}\".format(nll))\n",
        "    print(\"Perplexity <= {:0.3f}\".format(perplexity))\n",
        "\n",
        "    # Print top 10 words for first 10 topics.\n",
        "    qW0_vals = sess.run(qW0)\n",
        "    for k in range(10):\n",
        "        top_words_idx = qW0_vals[k, :].argsort()[-10:][::-1]\n",
        "        top_words = \" \".join([words[i] for i in top_words_idx])\n",
        "        print(\"Topic {}: {}\".format(k, top_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7cEGjAx5lojz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Visualizing the graph we've constructed\n",
        "# draw_graph(linear_mixed_effects_model, features_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VA1I7Jf-RHH5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Reference\n",
        "\n",
        "1. (Ranganath et al., 2015). [Deep Exponential Families](http://www.cs.toronto.edu/~lcharlin/papers/def_aistats.pdf)\n",
        "\n",
        "[1] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelligence, 35(8), 2013.\n",
        "\n",
        "[2] C. Bishop. Pattern Recognition and Machine Learning. Springer New York., 2006.\n",
        "\n",
        "[3] D. Blei and J. Lafferty. Dynamic topic models. In International Conference on Machine Learning, 2006.\n",
        "\n",
        "[4] D. Blei and J. Lafferty. A correlated topic model of Science. Annals of Applied Stat., 1(1):17–35, 2007.\n",
        "\n",
        "[5] D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum. Hierarchical topic models and the nested Chinese restaurant process. In NIPS, 2003.\n",
        "\n",
        "[6] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, 3: 993–1022, January 2003.\n",
        "\n",
        "[7] L. Brown. Fundamentals of Statistical Exponential Families. Institute of Mathematical Statistics, 1986.\n",
        "\n",
        "[8] W. Buntine and A. Jakulin. Applying discrete PCA in data analysis. In Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence, pages 59–66. AUAI Press, 2004.\n",
        "\n",
        "[9] J. Canny. GaP: A factor model for discrete data. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2004.\n",
        "\n",
        "[10] A. Gelman and J. Hill. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge Univ. Press, 2007.\n",
        "\n",
        "[11] Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. Bayesian data analysis. CRC press, 2013.\n",
        "\n",
        "[12] I. Goodfellow, A. Courville, and Y. Bengio. Largescale feature learning with spike-and-slab sparse coding. In International Conference on Machine Learning (ICML), 2012.\n",
        "\n",
        "[13] P. Gopalan, J. Hoffman, and D. Blei. Scalable recommendation with poisson factorization. arXiv, (1311.1704), 2013.\n",
        "\n",
        "[14] Daniel Hern´andez-Lobato, Jos´e Miguel Hern´andezLobato, and Pierre Dupont. Generalized spike-andslab priors for bayesian group feature selection using expectation propagation. The Journal of Machine Learning Research, 14(1):1891–1945, 2013.\n",
        "\n",
        "[15] G. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural Comput., 18(7): 1527–1554, July 2006. ISSN 0899-7667.\n",
        "\n",
        "[16] H. Ishwaran and S. Rao. Spike and slab variable selection: Frequentist and Bayesian strategies. The Annals of Statistics, 33(2):730–773, 2005.\n",
        "\n",
        "[17] Kalervo J¨arvelin and Jaana Kek¨al¨ainen. Ir evaluation methods for retrieving highly relevant documents. In In International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’00, pages 41–48. ACM, 2000. ISBN 1-58113-226-3.\n",
        "\n",
        "[18] M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical models. Machine Learning, 37:183–233, 1999.\n",
        "\n",
        "[19] Hugo Larochelle and Stanislas Lauly. A neural autoregressive topic model. In Neural Information Processing Systems, 2012.\n",
        "\n",
        "[20] D. Lee and H. Seung. Learning the parts of objects by non-negative matrix factorization. Nature, 401(6755): 788–791, October 1999.\n",
        "\n",
        "[21] W. Li and A. McCallum. Pachinko allocation:DAGstuctured mixture models of topic correlations. In ICML, 2006.\n",
        "\n",
        "[22] Benjamin Marlin. Collaborative filtering: A machine learning perspective. Technical report, University of Toronto, 2004.\n",
        "\n",
        "[23] A. Mnih and K. Gregor. Neural variational inference and learning in belief networks. In ICML, 2014.\n",
        "\n",
        "[24] S. Mohamed, K. Heller, and Z. Ghahramani. Bayesian exponential family PCA. In NIPS, 2008.\n",
        "\n",
        "[25] R. Neal. Learning stochastic feedforward networks. Tech. Rep. CRG-TR-90-7: Department of Computer Science, University of Toronto, 1990.\n",
        "\n",
        "[26] J. A. Nelder and R. W. M. Wedderburn. Generalized linear models. Journal of the Royal Statistical Society. Series A (General), 135:370–384, 1972.\n",
        "\n",
        "[27] R. Ranganath, S. Gerrish, and D. Blei. Black box variational inference. In International Conference on Artifical Intelligence and Statistics, 2014.\n",
        "\n",
        "[28] D. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. ArXiv e-prints, January 2014.\n",
        "\n",
        "[29] H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics, 22(3): pp. 400–407, 1951.\n",
        "\n",
        "[30] R. Salakhutdinov and G. Hinton. Deep boltzmann machines. In AISTATS, pages 448–455, 2009.\n",
        "\n",
        "[31] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov chain Monte Carlo. In International Conference on Machine learning, 2008.\n",
        "\n",
        "[32] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. In Neural Information Processing Systems, 2008.\n",
        "\n",
        "[33] Ruslan Salakhutdinov and Geoffrey Hinton. Replicated softmax: an undirected topic model. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 1607–1614. 2009.\n",
        "\n",
        "[34] T. Salimans and D. Knowles. Fixed-form variational posterior approximation through stochastic linear regression. Bayesian Analysis, 8(4):837–882, 2013\n",
        ".\n",
        "[35] L. Saul, T. Jaakkola, and M. Jordan. Mean field theory for sigmoid belief networks. Journal of Artificial Intelligence Research, 4:61–76, 1996."
      ]
    },
    {
      "metadata": {
        "id": "rQz8re8LRHH6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.core.display import HTML\n",
        "def css_styling():\n",
        "    styles = open(\"../styles/custom.css\", \"r\").read()\n",
        "    return HTML(styles)\n",
        "css_styling()\n",
        "\n",
        "#  \"#F15854\",  // red\n",
        "#  \"#5DA5DA\",  // blue\n",
        "#  \"#FAA43A\",  // orange\n",
        "#  \"#60BD68\",  // green\n",
        "#  \"#F17CB0\",  // pink\n",
        "#  \"#B2912F\",  // brown\n",
        "#  \"#B276B2\",  // purple\n",
        "#  \"#DECF3F\",  // yellow\n",
        "#  \"#4D4D4D\",  // gray"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}