{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JJ3UDciDVcB5"
      },
      "source": [
        "# Bayesian Gaussian Mixture Model and Hamiltonian MCMC\n",
        "\n",
        "In this colab we'll explore sampling from the posterior of a Bayesian Gaussian Mixture Model (BGMM) using only Tensorflow Probability primitives. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eZs1ShikNBK2"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7JjokKMbk2hJ"
      },
      "source": [
        "For $k\\in\\{1,\\ldots, K\\}$ mixture components each of dimension $D$, we'd like to model $i\\in\\{1,\\ldots,N\\}$ iid samples using the following Bayesian Gaussian Mixture Model:\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\theta \u0026\\sim \\text{Dirichlet}(\\text{concentration}=\\alpha_0) \\\\\n",
        "\\mu_k \u0026\\sim \\text{Normal}(\\text{loc}=\\mu_{0k}, \\text{scale}=I_D) \\\\\n",
        "T_k \u0026\\sim \\text{Wishart}(\\text{df}=5, \\text{scale}=I_D) \\\\\n",
        "Z_i \u0026\\sim \\text{Categorical}(\\text{probs}=\\theta) \\\\\n",
        "Y_i \u0026\\sim \\text{Normal}(\\text{loc}=\\mu_{z_i}, \\text{scale}=T_{z_i}^{-1/2})\\\\\n",
        "\\end{align*}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iySRABi0qZnQ"
      },
      "source": [
        "Note, the `scale` arguments all have `cholesky` semantics. We use this convention because it is that of TF Distributions (which itself uses this convention in part because it is computationally advantageous). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y6X_Beihwzyi"
      },
      "source": [
        "Our goal is to generate samples from the posterior:\n",
        "\n",
        "$$p\\left(\\theta, \\{\\mu_k, T_k\\}_{k=1}^K \\Big| \\{y_i\\}_{i=1}^N, \\alpha_0, \\{\\mu_{ok}\\}_{k=1}^K\\right)$$\n",
        "\n",
        "Notice that $\\{Z_i\\}_{i=1}^N$ is not present--we're interested in only those random variables which don't scale with $N$.  (And luckily there's a TF distribution which handles marginalizing out $Z_i$.)\n",
        "\n",
        "It is not possible to directly sample from this distribution owing to a computationally intractable normalization term. \n",
        "\n",
        "[Metropolis-Hastings algorithms](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) are technique for for sampling from intractable-to-normalize distributions.\n",
        "\n",
        "Tensorflow Probability offers a number of MCMC options, including several based on Metropolis-Hastings. In this notebook, we'll use [Hamiltonian Monte Carlo](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo)  (`tfp.mcmc.HamiltonianMonteCarlo`). HMC is often a good choice because it can converge rapidly, samples the state space jointly (as opposed to coordinatewise), and leverages one of TF's virtues: automatic differentiation. That said, fitting a BGMM might actually be better served by other approaches, e.g., [Gibb's sampling](https://en.wikipedia.org/wiki/Gibbs_sampling)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "uswTWdgNu46j"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import matplotlib.pyplot as plt; plt.style.use('ggplot')\n",
        "import numpy as np\n",
        "import seaborn as sns; sns.set_context('notebook')\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow as tf \n",
        "\n",
        "from tensorflow.python.ops.distributions import util as distribution_util\n",
        "\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.distributions.bijectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "ovNsKD-OEUzR"
      },
      "outputs": [],
      "source": [
        "def session_options(enable_gpu_ram_resizing=True):\n",
        "  \"\"\"Convenience function which sets common `tf.Session` options.\"\"\"\n",
        "  config = tf.ConfigProto()\n",
        "  config.log_device_placement = True\n",
        "  if enable_gpu_ram_resizing:\n",
        "    # `allow_growth=True` makes it possible to connect multiple colabs to your\n",
        "    # GPU. Otherwise the colab malloc's all GPU ram.\n",
        "    config.gpu_options.allow_growth = True\n",
        "  return config\n",
        "\n",
        "def reset_sess(config=None):\n",
        "  \"\"\"Convenience function to create the TF graph and session, or reset them.\"\"\"\n",
        "  if config is None:\n",
        "    config = session_options()\n",
        "  tf.reset_default_graph()\n",
        "  global sess\n",
        "  try:\n",
        "    sess.close()\n",
        "  except:\n",
        "    pass\n",
        "  sess = tf.InteractiveSession(config=config)\n",
        "\n",
        "reset_sess()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Uj9uHZN2yUqz"
      },
      "source": [
        "Before actually building the model, we'll need to define a new type of distribution.  From the model specification above, its clear we're parameterizing the MVN with an inverse covariance matrix, i.e.,  [precision matrix](https://en.wikipedia.org/wiki/Precision_(statistics%29).  To accomplish this in TF,  we'll use the transformation `tfb.Invert(tfb.Affine(...))`. In so doing the `log_prob` calculation uses [`tf.matmul`](https://www.tensorflow.org/api_docs/python/tf/matmul) instead of [`tf.linalg.triangular_solve`](https://www.tensorflow.org/api_docs/python/tf/matrix_triangular_solve) (as would be the case for `tfd.MultivariateNormalTriL`). Aside from this being arguably more interprettable, using `tf.matmul` is advantageous since it is usually faster owing to better cache locality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "Zv_GfWWkvNfi"
      },
      "outputs": [],
      "source": [
        "class MVNInverseTriL(tfd.TransformedDistribution):\n",
        "  \"\"\"MVN from loc and (Cholesky) precision matrix.\"\"\"\n",
        "\n",
        "  def __init__(self, loc, precision_tril, name=None):\n",
        "    super(MVNInverseTriL, self).__init__(\n",
        "        distribution=tfd.Independent(tfd.Normal(loc, scale=tf.ones_like(loc)),\n",
        "                                     reinterpreted_batch_ndims=1),\n",
        "        bijector=tfb.Invert(tfb.Affine(scale_tril=precision_tril)),\n",
        "        name=name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JDOkWhDQg4ZG"
      },
      "source": [
        "The `tfd.Independent` distribution turns independent draws of one distribution, into a multivariate distribution with statistically independent coordinates. In terms of computing `log_prob`, this \"meta-distribution\" manifests as a simple sum over the event dimension(s)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N60z8scN1v6E"
      },
      "source": [
        "Using the above definition, specifying the random variables is relatively easy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "xhzxySDjL2-S"
      },
      "outputs": [],
      "source": [
        "dtype = np.float32\n",
        "dims = 2\n",
        "components = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "xAOmHhZ7LzDQ"
      },
      "outputs": [],
      "source": [
        "rv_mix_probs = tfd.Dirichlet(\n",
        "    concentration=np.ones(components, dtype) / 10.,\n",
        "    name='rv_mix_probs')\n",
        "\n",
        "rv_loc = tfd.Independent(\n",
        "    tfd.Normal(\n",
        "        loc=np.stack([\n",
        "            -np.ones(dims, dtype),\n",
        "            np.zeros(dims, dtype),\n",
        "            np.ones(dims, dtype),\n",
        "        ]),\n",
        "        scale=tf.ones([components, dims], dtype)),\n",
        "    reinterpreted_batch_ndims=1,\n",
        "    name='rv_loc')\n",
        "\n",
        "rv_precision = tfd.WishartCholesky(\n",
        "    df=5,\n",
        "    scale=np.stack([np.eye(dims, dtype=dtype)]*components),\n",
        "    cholesky_input_output_matrices=True,\n",
        "    name='rv_precision')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 71
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 15,
          "status": "ok",
          "timestamp": 1528915184138,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "KSTp8aAIAv0O",
        "outputId": "84246560-84d2-42ce-dd3e-f153e4c33cd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.distributions.Dirichlet(\"rv_mix_probs/\", batch_shape=(), event_shape=(3,), dtype=float32)\n",
            "tf.distributions.Independent(\"rv_loc/\", batch_shape=(3,), event_shape=(2,), dtype=float32)\n",
            "tf.distributions.WishartCholesky(\"rv_precision/\", batch_shape=(3,), event_shape=(2, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "print(rv_mix_probs)\n",
        "print(rv_loc)\n",
        "print(rv_precision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8ZOG0OR815Nr"
      },
      "source": [
        "Using the three random variables defined above, we can now specify the joint log probability function. To do this we'll use `tfd.MixtureSameFamily` to automatically integrate out the categorical $\\{Z_i\\}_{i=1}^N$ draws."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "CpLnRJr2TXYD"
      },
      "outputs": [],
      "source": [
        "def joint_log_prob(observations, mix_probs, loc, chol_precision):\n",
        "  \"\"\"BGMM with priors: loc=Normal, precision=Inverse-Wishart, mix=Dirichlet.\n",
        "\n",
        "  Args:\n",
        "    observations: `[n, d]`-shaped `Tensor` representing Bayesian Gaussian\n",
        "      Mixture model draws. Each sample is a length-`d` vector.\n",
        "    mix_probs: `[K]`-shaped `Tensor` representing random draw from\n",
        "      `SoftmaxInverse(Dirichlet)` prior.\n",
        "    loc: `[K, d]`-shaped `Tensor` representing the location parameter of the\n",
        "      `K` components.\n",
        "    chol_precision: `[K, d, d]`-shaped `Tensor` representing `K` lower\n",
        "      triangular `cholesky(Precision)` matrices, each being sampled from\n",
        "      a Wishart distribution.\n",
        "\n",
        "  Returns:\n",
        "    log_prob: `Tensor` representing joint log-density over all inputs.\n",
        "  \"\"\"\n",
        "  rv_observations = tfd.MixtureSameFamily(\n",
        "      mixture_distribution=tfd.Categorical(probs=mix_probs),\n",
        "      components_distribution=MVNInverseTriL(\n",
        "          loc=loc,\n",
        "          precision_tril=chol_precision))\n",
        "  log_prob_parts = [\n",
        "      rv_observations.log_prob(observations), # Sum over samples.\n",
        "      rv_mix_probs.log_prob(mix_probs)[..., tf.newaxis], \n",
        "      rv_loc.log_prob(loc),                   # Sum over components.\n",
        "      rv_precision.log_prob(chol_precision),  # Sum over components.\n",
        "  ]\n",
        "  sum_log_prob = tf.reduce_sum(tf.concat(log_prob_parts, axis=-1), axis=-1)\n",
        "  # Note: for easy debugging, uncomment the following:\n",
        "  # sum_log_prob = tf.Print(sum_log_prob, log_prob_parts)\n",
        "  return sum_log_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QM1idLJazkGC"
      },
      "source": [
        "Notice that this function internally defines a new random variable. This is necessary since the `observations` RV depends on samples from the RVs defined further above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7jTMXdymV1QJ"
      },
      "source": [
        "## Generate \"Training\" Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rl4brz3G3pS7"
      },
      "source": [
        "For this demo, we'll sample some random data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "1AJZAtwXV8RQ"
      },
      "outputs": [],
      "source": [
        "num_samples = 1000\n",
        "true_loc = np.array([[-2, -2],\n",
        "                     [0, 0],\n",
        "                     [2, 2]], dtype)\n",
        "random = np.random.RandomState(seed=42)\n",
        "\n",
        "true_hidden_component = random.randint(0, components, num_samples)\n",
        "observations = (true_loc[true_hidden_component] +\n",
        "                random.randn(num_samples, dims).astype(dtype))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zVOvMh7MV37A"
      },
      "source": [
        "## Bayesian Inference using HMC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cdN3iKFT32Jp"
      },
      "source": [
        "Now that we've used TFD to specify our model and obtained some observed data, we have all the necessary pieces to run HMC.\n",
        "\n",
        "To do this, we'll use a [closure](https://en.wikipedia.org/wiki/Closure_(computer_programming%29#Anonymous_functions) to \"pin down\" the things we don't want to sample. In this case that means we need only pin down `observations`. (The hyper-parameters are already baked in to the prior distributions and not part of the `joint_log_prob` function signature.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "tVoaDFSf7L_j"
      },
      "outputs": [],
      "source": [
        "unnormalized_posterior_log_prob = lambda *args: joint_log_prob(observations, *args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "a0OMIWIYeMmQ"
      },
      "outputs": [],
      "source": [
        "initial_state = [\n",
        "    tf.fill([components],\n",
        "            value=np.array(1. / components, dtype),\n",
        "            name='mix_probs'),\n",
        "    tf.constant(np.array([[-2, -2],\n",
        "                          [0, 0],\n",
        "                          [2, 2]], dtype),\n",
        "                name='loc'),\n",
        "    tf.eye(dims, batch_shape=[components], dtype=dtype, name='chol_precision'),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TVpiT3LLyfcO"
      },
      "source": [
        "### Unconstrained Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JS8XOsxiyiBV"
      },
      "source": [
        "Hamiltonian Monte Carlo (HMC) requires the target log-probability function be differentiable with respect to its arguments.  Furthermore, HMC can exhibit dramatically higher statistical efficiency if the state-space is unconstrained.\n",
        "\n",
        "This means we'll have to work out two main issues when sampling from the BGMM posterior:\n",
        "\n",
        "1. $\\theta$ represents a discrete probability vector, i.e., must be such that $\\sum_{k=1}^K \\theta_k = 1$ and $\\theta_k\u003e0$.\n",
        "2. $T_k$ represents an inverse covariance matrix, i.e., must be such that $T_k \\succ 0$, i.e., is [positive definite](https://en.wikipedia.org/wiki/Positive-definite_matrix).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vt9SXJzO0Cks"
      },
      "source": [
        "To address this requirement we'll need to:\n",
        "\n",
        "1. transform the constrained variables to an unconstrained space\n",
        "2. run the MCMC in unconstrained space\n",
        "3. transform the unconstrained variables back to the constrained space.\n",
        "\n",
        "As with `MVNInverseTriL`, we'll use [`Bijector`s](https://www.tensorflow.org/api_docs/python/tf/distributions/bijectors/Bijector) to transform random variables to unconstrained space.\n",
        "\n",
        "- The [`Dirichlet`](https://en.wikipedia.org/wiki/Dirichlet_distribution) is transformed to unconstrained space via the [softmax function](https://en.wikipedia.org/wiki/Softmax_function).\n",
        "\n",
        "- Our precision random variable is a distribution over postive semidefinite matrices. To unconstrain these we'll use the `FillTriangular` and `TransformDiagonal` bijectors.  These convert vectors to lower-triangular matrices and ensure the diagonal is positive. The former is useful because it enables sampling only $d(d+1)/2$ floats rather than $d^2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "_atEQrDR7JvG"
      },
      "outputs": [],
      "source": [
        "unconstraining_bijectors = [\n",
        "    tfb.SoftmaxCentered(),\n",
        "    tfb.Identity(),\n",
        "    tfb.Chain([\n",
        "        tfb.TransformDiagonal(tfb.Softplus()),\n",
        "        tfb.FillTriangular(),\n",
        "    ])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "0zq6QJJ-NSPJ"
      },
      "outputs": [],
      "source": [
        "[mix_probs, loc, chol_precision], kernel_results = tfp.mcmc.sample_chain(\n",
        "    num_results=2000,\n",
        "    num_burnin_steps=500,\n",
        "    current_state=initial_state,\n",
        "    kernel=tfp.mcmc.TransformedTransitionKernel(\n",
        "        inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
        "            target_log_prob_fn=unnormalized_posterior_log_prob,\n",
        "            step_size=0.03,\n",
        "            num_leapfrog_steps=5),\n",
        "        bijector=unconstraining_bijectors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "_ceX1A3-ZFiN"
      },
      "outputs": [],
      "source": [
        "acceptance_rate = tf.reduce_mean(tf.to_float(kernel_results.inner_results.is_accepted))\n",
        "mean_mix_probs = tf.reduce_mean(mix_probs, axis=0)\n",
        "mean_loc = tf.reduce_mean(loc, axis=0)\n",
        "mean_chol_precision = tf.reduce_mean(chol_precision, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kmpTFZcVmByb"
      },
      "source": [
        "Note: we've already tuned the `step_size` and `num_leapfrog_steps` to approximately achieve an [asymptotically optimal rate of 0.651](https://arxiv.org/abs/1001.4460)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QLEz96mg6fpZ"
      },
      "source": [
        "We'll now execute the chain and print the posterior means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "colab_type": "code",
        "id": "3B2yJWVmNcrm"
      },
      "outputs": [],
      "source": [
        "[\n",
        "    acceptance_rate_,\n",
        "    mean_mix_probs_,\n",
        "    mean_loc_,\n",
        "    mean_chol_precision_,\n",
        "    mix_probs_,\n",
        "    loc_,\n",
        "    chol_precision_,\n",
        "] = sess.run([\n",
        "    acceptance_rate,\n",
        "    mean_mix_probs,\n",
        "    mean_loc,\n",
        "    mean_chol_precision,\n",
        "    mix_probs,\n",
        "    loc,\n",
        "    chol_precision,\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 323
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 14,
          "status": "ok",
          "timestamp": 1528915230839,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "bqJ6RSJxegC6",
        "outputId": "1c80d705-ddb3-46a8-9f96-14cbf2d5c9b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    acceptance_rate: 0.641\n",
            "      avg mix probs: [ 0.39652753  0.23796532  0.36550719]\n",
            "\n",
            "            avg loc:\n",
            " [[-1.85150051 -1.58520997]\n",
            " [-0.01392753  0.01626917]\n",
            " [ 1.81968725  1.59268594]]\n",
            "\n",
            "avg chol(precision):\n",
            " [[[ 0.99440396  0.        ]\n",
            "  [-0.07709358  0.96896082]]\n",
            "\n",
            " [[ 1.21073735  0.        ]\n",
            "  [ 0.31398559  1.11340523]]\n",
            "\n",
            " [[ 0.96760911  0.        ]\n",
            "  [-0.13563943  0.96555084]]]\n"
          ]
        }
      ],
      "source": [
        "print('    acceptance_rate:', acceptance_rate_)\n",
        "print('      avg mix probs:', mean_mix_probs_)\n",
        "print('\\n            avg loc:\\n', mean_loc_)\n",
        "print('\\navg chol(precision):\\n', mean_chol_precision_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "height": 286
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 4884,
          "status": "ok",
          "timestamp": 1528915236719,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "zFOU0j9kPdUy",
        "outputId": "6d195c9c-dd1c-410c-8257-8b5c90463fce"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAENCAYAAAASUO4dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "\u003cmatplotlib.figure.Figure at 0x7f65998246d0\u003e"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "ax = sns.kdeplot(loc_[:,0,0], loc_[:,0,1], shade=True)\n",
        "ax = sns.kdeplot(loc_[:,1,0], loc_[:,1,1], shade=True)\n",
        "ax = sns.kdeplot(loc_[:,2,0], loc_[:,2,1], shade=True)\n",
        "plt.title('KDE of loc draws');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NmfNIM1c6mwc"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t8LeIeMn6ot4"
      },
      "source": [
        "This simple colab demonstrated how Tensorflow Probability primitives can be used to build hierarchical Bayesian mixture models."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "default_view": {},
      "name": "Bayesian_Gaussian_Mixture_Model.ipynb",
      "provenance": [
        {
          "file_id": "1rxhvVh5S5WeWnyEBHqTiH_z0oGZVSGyx",
          "timestamp": 1527714835004
        }
      ],
      "version": "0.3.2",
      "views": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
